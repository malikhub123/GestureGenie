# âœ¨ GestureGenie â€“ Real-Time Hand Gesture to Text and Speech Converter

[![Python](https://img.shields.io/badge/Python-3.11-blue?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.12-orange?style=for-the-badge&logo=tensorflow&logoColor=white)](https://www.tensorflow.org/)
[![Flask](https://img.shields.io/badge/Flask-2.0-black?style=for-the-badge&logo=flask&logoColor=white)](https://flask.palletsprojects.com/)
[![OpenCV](https://img.shields.io/badge/OpenCV-4.8-green?style=for-the-badge&logo=opencv&logoColor=white)](https://opencv.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)

A **computer vision and deep learning** project that recognizes **static hand gestures** of the English alphabet (Aâ€“Z) from a webcam feed and converts them into **text and speech** in real-time.

---


## ğŸŒŸ Overview

GestureGenie uses a custom-trained **Convolutional Neural Network (CNN)** to detect hand gestures, predict the corresponding letter, and enable users to build words and hear them spoken aloud through an interactive web interface.

Built completely **from scratch** â€“ from data collection and preprocessing to model training and real-time deployment using Flask.

---

## ğŸš€ Demo

https://github.com/user-attachments/assets/459b6252-e8b8-4959-8bc7-1dc1b801a0f6

---

## âœ¨ Key Features

- ğŸ¯ **Real-Time Gesture Recognition** â€“ Detects hand gestures from webcam feed
- ğŸ”¤ **A-Z Alphabet Support** â€“ Recognizes all 26 English alphabet letters
- ğŸ—£ï¸ **Text-to-Speech** â€“ Converts built words to speech using gTTS
- ğŸ–¥ï¸ **Interactive Web Interface** â€“ User-friendly Flask-based UI
- ğŸ¤– **Custom CNN Model** â€“ Trained from scratch on self-collected dataset
- ğŸ“¸ **Live Video Streaming** â€“ Real-time prediction overlay on webcam feed
- âŒ¨ï¸ **Word Building** â€“ Add letters, spaces, delete, and speak words

---




### How It Works:

1. **Show a hand gesture** to the webcam
2. **GestureGenie detects** the hand using MediaPipe
3. **CNN predicts** the corresponding letter
4. **Build words** using the predicted letters
5. **Convert to speech** and hear your word!

---

## ğŸ› ï¸ Tech Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Deep Learning** | TensorFlow/Keras | Custom CNN model training and inference |
| **Computer Vision** | OpenCV | Image capture and preprocessing |
| **Hand Detection** | MediaPipe (cvzone) | Real-time hand tracking and detection |
| **Web Framework** | Flask | Backend server and live video streaming |
| **Text-to-Speech** | gTTS | Convert text to audio |
| **Audio Playback** | pygame | Play generated speech audio |
| **Frontend** | HTML, CSS, JavaScript | Interactive user interface |
| **Data Processing** | NumPy | Array operations and image manipulation |

---

## ğŸ“Š Project Pipeline

### 1ï¸âƒ£ Data Collection
- Custom Python script using **OpenCV** and **MediaPipe** (via cvzone HandDetector)
- Captured hand images from webcam with bounding box detection
- Cropped and centered hand on white square background
- Organized into separate folders for each letter:
  ```
  Data/A, Data/B, Data/C, ..., Data/Z
  ```
- **900-1000 images** collected per letter

### 2ï¸âƒ£ Data Preprocessing
Each captured hand image was:
- âœ‚ï¸ Cropped tightly around the hand
- ğŸ“ Resized proportionally
- ğŸ¨ Placed on a fixed white canvas
- ğŸ”„ Resized to **128Ã—128** during training
- ğŸ”¢ Normalized to pixel range **[0, 1]**

### 3ï¸âƒ£ Model Training
- Custom **CNN architecture** built with TensorFlow/Keras
- Dataset split:
  - **80% Training**
  - **20% Validation**
- **26 output classes** (Aâ€“Z)
- Training with data augmentation (rotation, zoom, flip)

### 4ï¸âƒ£ Real-Time Prediction
- Same preprocessing pipeline applied to live webcam frames
- Processed image passed to trained model (`gesture_model.keras`)
- Model outputs probabilities for all 26 letters
- Highest probability letter displayed on video feed

### 5ï¸âƒ£ Word Building & Speech
- **Flask web app** streams live webcam feed
- Interactive controls:
  - â• Add predicted letter to word
  - âµ Insert space
  - âŒ« Delete last letter
  - ğŸ”Š Convert word to speech

---

## ğŸ§  Model Architecture

```
Input: 128Ã—128Ã—3 RGB Image
    â†“
Data Augmentation (Flip, Rotation, Zoom)
    â†“
Rescaling (1/255)
    â†“
Conv2D (32 filters, 3Ã—3) + ReLU + MaxPooling
    â†“
Conv2D (64 filters, 3Ã—3) + ReLU + MaxPooling
    â†“
Conv2D (128 filters, 3Ã—3) + ReLU + MaxPooling
    â†“
GlobalAveragePooling2D
    â†“
Dense (128 units, ReLU)
    â†“
Dropout (0.5)
    â†“
Dense (26 units, Softmax) â†’ Output [A-Z]
```

**Loss Function:** `sparse_categorical_crossentropy`  
**Optimizer:** `Adam`  
**Metric:** `Accuracy`  

**Trained model saved as:** `gesture_model.keras`

---

## ğŸ“ Project Structure

```
Gesture-Genie/
â”‚
â”œâ”€â”€ DataCollection.py        # Script to collect hand gesture images
â”œâ”€â”€ app.py                   # Flask-based real-time prediction app
â”œâ”€â”€ keras_model.h5           # Trained model (generated externally)
â”œâ”€â”€ labels.txt               # Class labels (Aâ€“Z)
â”œâ”€â”€ requirements.txt         # Project dependencies
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html           # Frontend UI
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation & Setup

### Prerequisites
- **Python 3.8+** installed on your system
- Webcam for real-time gesture detection

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/your-username/Gesture-Genie.git
cd Gesture-Genie
```

### 2ï¸âƒ£ Install Dependencies

Install all required libraries:

```bash
pip install -r requirements.txt
```

**Dependencies include:**
```txt
tensorflow==2.12.1
opencv-python
mediapipe
cvzone
flask
gTTS
pygame
numpy
```

### 3ï¸âƒ£ (Optional) Create Your Own Dataset

If you want to collect your own gesture images:

```bash
python DataCollection.py
```

**Instructions:**
- A webcam window will open
- Show the hand gesture for the desired letter
- Press `S` to save images
- Change the folder name in the script for each letter (e.g., `Data/A`, `Data/B`, ..., `Data/Z`)
- Collect 900-1000 images per letter for best results

âš ï¸ **Note:** This step is optional if you already have a trained model.

### 4ï¸âƒ£ Train the Model (Optional)

If you collected your own dataset:

```bash
python train_model.py
```

This will:
- Load images from the `Data/` directory
- Train the CNN model
- Save the trained model as `gesture_model.keras`

### 5ï¸âƒ£ Run the Application (if not collected your own dataset)

Start the Flask server:

```bash
python app.py
```

### 6ï¸âƒ£ Open the Web Interface

Open your browser and navigate to:

```
http://127.0.0.1:5000
```

---

## ğŸ® How to Use

1. **Allow camera access** when prompted
2. **Show hand gestures** (A-Z) to the webcam
3. **See real-time predictions** on the video feed
4. **Click "Add Letter"** to add the predicted letter to your word
5. **Use controls:**
   - **Space** â€“ Add a space between words
   - **Delete** â€“ Remove the last character
   - **Speak** â€“ Hear your word/sentence pronounced
6. **Build sentences** and have fun! ğŸ‰

---

## ğŸ“ˆ Model Performance

| Metric | Score |
|--------|-------|
| **Training Accuracy** | ~95%+ |
| **Validation Accuracy** | ~92%+ |
| **Real-Time FPS** | 15-25 FPS |
| **Inference Time** | ~50-80ms per frame |

> Results may vary based on lighting conditions and hand positioning

---


## ğŸš§ Challenges & Solutions

### Challenge 1: Inconsistent Hand Detection
**Solution:** Used MediaPipe with cvzone HandDetector for robust hand tracking and bounding box extraction.

### Challenge 2: Background Noise
**Solution:** Preprocessed images with consistent white background and proper cropping to reduce background interference.

### Challenge 3: Model Overfitting
**Solution:** Implemented data augmentation (rotation, zoom, flip) and dropout layers to improve generalization.

### Challenge 4: Real-Time Performance
**Solution:** Optimized image preprocessing pipeline and used GlobalAveragePooling instead of Flatten to reduce parameters.

---

## ğŸ”® Future Enhancements

- [ ] ğŸ¯ Add support for **dynamic gestures** (continuous sign language)
- [ ] ğŸŒ Support for **multiple languages**
- [ ] ğŸ“± **Mobile application** (Android/iOS)
- [ ] ğŸ¨ Improved **UI/UX design**
- [ ] ğŸ“Š **Real-time accuracy metrics** display
- [ ] ğŸ”„ **Model retraining** feature from the interface
- [ ] ğŸ¥ **Video-to-text** conversion for pre-recorded videos
- [ ] ğŸ§  **Advanced models** (LSTM for sentence prediction)
- [ ] â˜ï¸ **Cloud deployment** (Heroku, AWS, or Google Cloud)

---

## ğŸ“š Learning Outcomes

This project demonstrates:

âœ… End-to-end **deep learning pipeline**  
âœ… **Computer vision** with OpenCV and MediaPipe  
âœ… Custom **CNN architecture** design  
âœ… **Data collection and preprocessing** techniques  
âœ… **Real-time inference** and deployment  
âœ… **Flask web development**  
âœ… Integration of **third-party APIs** (gTTS)  
âœ… **Model evaluation and optimization**  

---


## ğŸ‘©â€ğŸ’» Author

**Aditi Malik**  
*B.Tech â€“ Computer Science & Engineering*  
*Full-Stack Developer | AI/ML Enthusiast*

[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://linkedin.com/in/aditi-malik-43880a222/)
[![Email](https://img.shields.io/badge/Email-D14836?style=for-the-badge&logo=gmail&logoColor=white)](mailto:malik2002.aditi@gmail.com)

---

## ğŸ“ Contact

For questions, suggestions, or collaboration:
- ğŸ“§ Email: malik2002.aditi@gmail.com
- ğŸ’¼ LinkedIn: [Your Profile](https://linkedin.com/in/aditi-malik-43880a222/)
- ğŸ™ GitHub: [@your-username](https://github.com/malikhub123)

---

## â­ Show Your Support

If you found this project helpful or interesting, please consider giving it a star! â­

---

<div align="center">

**Built with â¤ï¸ and lots of hand gestures ğŸ‘‹**

Made by Aditi Malik | 2025

</div>
